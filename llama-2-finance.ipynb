{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f78bf98-a0d9-4b7c-976c-1b8f7e0e6e88",
   "metadata": {},
   "source": [
    "## 1: Create model response function\n",
    "The create_model_response function is designed to generate a textual response from a given inference model based on a user's query. It formats the query into a template that the model understands, encodes this input, and then decodes the model's output into a human-readable response.\n",
    "\n",
    "### Parameters\n",
    "- `task_query (str)`: The user's query or task description that needs to be responded to.\n",
    "- `inference_model`: The machine learning model that generates the response. This should be pre-loaded and ready for inference.\n",
    "- `sequence_tokenizer`: A tokenizer that corresponds to the `inference_model`, used for both encoding the input and decoding the output.\n",
    "\n",
    "### Returns\n",
    "`str`: A string representing the model's response to the `task_query`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207c99e-6405-449c-9fcf-2973e646a8d7",
   "metadata": {},
   "source": [
    "## 2: Install Packages\n",
    "This script is a sequence of commands to install and update various Python packages, primarily focused on machine learning and data handling, using pip, the Python package installer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744e0be-3f03-4882-9526-b8fc6c3cfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U bitsandbytes\n",
    "\n",
    "!pip install  -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install ipywidgets\n",
    "\n",
    "#!pip install trl\n",
    "#!pip install  -U git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "!pip install transformers==4.36.0\n",
    "!pip install trl==0.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d5005-d6a7-4f26-b588-aa34d4d5c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_response(task_query: str, inference_model, sequence_tokenizer) -> str:\n",
    "    processing_device = \"cuda:0\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
    "\n",
    "    ### Instruction:\n",
    "    {user_task_query}\n",
    "\n",
    "    ### Completion:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the template with the user's query\n",
    "    task_prompt = prompt_template.format(user_task_query=task_query)\n",
    "\n",
    "    # Encoding the prompt for the model\n",
    "    encoded_input = sequence_tokenizer(task_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Sending the encoded input to the designated processing device\n",
    "    model_input_tensor = encoded_input.to(processing_device)\n",
    "\n",
    "    # Generating tokens from the model based on the input\n",
    "    generated_token_ids = inference_model.generate(\n",
    "        **model_input_tensor, \n",
    "        max_new_tokens=1000, \n",
    "        do_sample=True, \n",
    "        pad_token_id=sequence_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decoding the generated tokens to form the response\n",
    "    generated_response = sequence_tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_response[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31579ff4-ad82-48a2-bd87-f89b4e4a3198",
   "metadata": {},
   "source": [
    "### 3: System and CUDA Check\n",
    "This code helps you determine your PyTorch version and assess the availability of CUDA (GPU acceleration). If CUDA is accessible, it offers insights into your GPUs, including names, properties, and memory usage. In case CUDA is not available, it informs you that no GPU has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7ca9-729e-43cb-8308-b02bbdcf8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"Number of GPU:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Current Device : {torch.cuda.current_device}\")\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        print(f\"Memory GB: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"GPU Cached:    {torch.cuda.memory_reserved(i) / 1024 ** 3:.2f} GB\")\n",
    "        \n",
    "        \n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "\n",
    "print(torch.cuda._get_device_index)\n",
    "print(torch.cuda.current_device)\n",
    "\n",
    "#torch.cuda.set_device(3)\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(CUDA_VISIBLE_DEVICES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ccc12-d1b5-4bd9-920f-27cee044a335",
   "metadata": {},
   "source": [
    "This command, `!nvidia-smi`, provides a concise overview of NVIDIA GPU information, displaying details like GPU model, memory usage, and temperature in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76acbdc-a175-4d6d-bcbd-d6a6915155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7ff83-25eb-480b-9972-686a1bdf33cf",
   "metadata": {},
   "source": [
    "## 4: Load and Display Dataset\n",
    "This section of the code leverages the Hugging Face datasets library to import a financial dataset from Alpaca. It then transforms this dataset into a Pandas DataFrame for more flexible data handling. To ensure comprehensive visibility, the display settings of Pandas are tweaked to show all columns and the entire content of each cell. The code culminates by showcasing the first 10 rows of the DataFrame, offering a preliminary glimpse into the dataset's structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308d105-58ae-43ac-9ee4-6439ccaebc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)  # Show all columns in the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure the full content of each cell is displayed\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"gbharti/finance-alpaca\", split='train')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb27c76-8e47-4abc-8578-1c9aa9751926",
   "metadata": {},
   "source": [
    "## 5: Data Analysis and Visualization\n",
    "This code uses Pandas and Matplotlib to provide a comprehensive analysis of a DataFrame. It includes basic information such as the number of rows, columns, and column names, memory usage, data types, and descriptive statistics for character counts in specific columns. It also displays distributions and additional detailed statistics like missing values and unique values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dae57-6c9c-45de-8c99-a07e285283f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and it's already loaded\n",
    "\n",
    "# Basic Dataset Information\n",
    "print(\"Basic Dataset Information:\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(f\"Column Names: {df.columns.tolist()}\", end=\"\\n\\n\")\n",
    "\n",
    "# Memory Usage\n",
    "print(\"Memory Usage by Column:\")\n",
    "print(df.memory_usage(deep=True), end=\"\\n\\n\")\n",
    "\n",
    "# Data Types\n",
    "print(\"Data Types of Each Column:\")\n",
    "print(df.dtypes, end=\"\\n\\n\")\n",
    "\n",
    "# Calculating the length of each cell in each column\n",
    "analysis_df = df.copy()\n",
    "analysis_df['num_characters_instruction'] = analysis_df['instruction'].apply(len)\n",
    "analysis_df['num_characters_input'] = analysis_df['input'].apply(len)\n",
    "analysis_df['num_characters_output'] = analysis_df['output'].apply(len)\n",
    "\n",
    "# Show Distribution\n",
    "analysis_df.hist(column=['num_characters_instruction', 'num_characters_input', 'num_characters_output'], bins=30, figsize=(12, 8))\n",
    "plt.suptitle('Distribution of Character Counts in Each Column')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive Statistics for Character Counts\n",
    "print(\"Descriptive Statistics for Character Counts:\")\n",
    "print(analysis_df[['num_characters_instruction', 'num_characters_input', 'num_characters_output']].describe(), end=\"\\n\\n\")\n",
    "\n",
    "# Additional Detailed Statistics\n",
    "max_chars_instruction = analysis_df['num_characters_instruction'].max()\n",
    "max_chars_input = analysis_df['num_characters_input'].max()\n",
    "max_chars_output = analysis_df['num_characters_output'].max()\n",
    "\n",
    "min_chars_instruction = analysis_df['num_characters_instruction'].min()\n",
    "min_chars_input = analysis_df['num_characters_input'].min()\n",
    "min_chars_output = analysis_df['num_characters_output'].min()\n",
    "\n",
    "# Print detailed statistics\n",
    "# Missing Values\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(analysis_df.isnull().sum(), end=\"\\n\\n\")\n",
    "\n",
    "# Unique Values\n",
    "print(\"Unique Values in Each Column:\")\n",
    "print(analysis_df.nunique(), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b91511-9a58-499c-be96-4d4032e6a085",
   "metadata": {},
   "source": [
    "## 6: Configure BitsAndBytes for Efficient Model Inference\n",
    "This code configures the `BitsAndBytes` feature in Hugging Face Transformers using `BitsAndBytesConfig`, enhancing model efficiency. The setup includes enabling 4-bit model loading, activating double quantization, setting the quantization type to `'nf4'`, and specifying `torch.bfloat16` as the compute data type. These configurations collectively aim to optimize memory usage and computational efficiency while balancing precision and speed for large model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac61500-8a23-44a5-ad87-5de1dd39ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig allows the configuration of the BitsAndBytes feature of Hugging Face Transformers.\n",
    "# This feature enables efficient model inference by reducing the model size and computational requirements.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enables loading the model in a 4-bit quantized format to reduce memory usage.\n",
    "    bnb_4bit_use_double_quant=True,  # Activates double quantization, which quantizes not just the weights but also the activations.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Sets the quantization type to 'nf4', a 4-bit number format for quantization.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Specifies bfloat16 as the data type for computation, balancing precision and speed.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa7932-9c0c-46fb-b72f-ef4ea3e05fce",
   "metadata": {},
   "source": [
    "## 7: Load Pre-Trained Model and Tokenizer\n",
    "This segment involves loading a pre-trained causal language model and its associated tokenizer from Hugging Face's model repository. The specific model loaded is identified as `\"meta-llama/Llama-2-7b-hf\"`. The model is configured for enhanced efficiency using the previously defined `BitsAndBytes` configuration. Additionally, the `device_map=\"auto\"` setting is used to automatically place the model on the most suitable computing device (CPU or GPU). For input text processing, the corresponding tokenizer is loaded with an added end-of-sentence token, ensuring proper tokenization of input sequences for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d35770-2d9e-4c8a-8f6f-a0fda446bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import login\n",
    "#login(token = 'my token')\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "token = \"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\"\n",
    "HfFolder.save_token(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7aae13-f176-4b35-8f97-459c38e359ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the identifier for the model we want to load from Hugging Face's model repository.\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load the pre-trained causal language model from Hugging Face with the specified model ID.\n",
    "# The model is configured for quantization using the previously defined BitsAndBytesConfig to improve efficiency.\n",
    "# 'device_map=\"auto\"' allows the model to be placed on the most appropriate device (CPU/GPU) automatically.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                  quantization_config=bnb_config, \n",
    "                                                  device_map=\"auto\")\n",
    "\n",
    "# Load the tokenizer associated with the pre-trained model.\n",
    "# The tokenizer is responsible for converting input text into a format that the model can understand (tokens).\n",
    "# 'add_eos_token=True' ensures that the end-of-sentence token is appended to the input sequences.\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136463-d922-4c49-b9ac-1a24592aa3e3",
   "metadata": {},
   "source": [
    "## 8: Generate and Display Model Response\n",
    "This code generates a language model completion for the given query using the previously initialized base_model and base_tokenizer. The result is then printed, showcasing the model's response to the input question about capital gains and tax brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153a46a-4104-4efe-81a8-d4c81c98aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response from the model for a specified query using the 'create_model_response' function.\n",
    "# The query here is \"Will capital gains affect my tax bracket?\"\n",
    "# 'base_model' is the loaded language model and 'base_tokenizer' is the corresponding tokenizer.\n",
    "# The function will process the query, generate a response using the model, and return the result as a string.\n",
    "result = create_model_response(task_query=\"Will capital gains affect my tax bracket?\",\n",
    "                               inference_model=base_model, \n",
    "                               sequence_tokenizer=base_tokenizer)\n",
    "\n",
    "# Print the result to display the model-generated response to the query.\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecd834-1f7f-4965-ab3e-2c9f812bd3b1",
   "metadata": {},
   "source": [
    "## 9. Function: create_contextual_prompt\n",
    "This function constructs a formatted prompt for a given task. It takes a dictionary containing key information about the task - specifically, an instruction, optional input context, and an expected output or response.\n",
    "\n",
    "### Parameters\n",
    "- `data_point (dict):` A dictionary with at least two keys: instruction and output, and an optional input key. These keys correspond to:\n",
    "- `instruction:` The task's instructions.\n",
    "- `input:` Optional additional context or information relevant to the task.\n",
    "- `output:` The expected response or result for the task.\n",
    "\n",
    "### Returns\n",
    "- `str:` A string that represents the complete, formatted prompt. This prompt is structured into sections labeled 'Instruction', 'Context' (if available), and 'Response'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8abb82-66ef-4384-b412-9043b86a4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a textual prompt incorporating instructions, optional context, and a response.\n",
    "\n",
    "    :param data_point: A dictionary containing instruction, optional input context, and output.\n",
    "    :return: A string representing a formatted and contextualized prompt.\n",
    "    \"\"\"\n",
    "    instruction = data_point[\"instruction\"]\n",
    "    input_context = data_point.get(\"input\")\n",
    "    response = data_point[\"output\"]\n",
    "\n",
    "    # Base prompt structure\n",
    "    text = 'This is a task instruction. Complete the task as described.\\n\\n'\n",
    "\n",
    "    # Adding instruction\n",
    "    text += f'### Instruction:\\n{instruction}\\n\\n'\n",
    "\n",
    "    # Conditionally adding input context if available\n",
    "    if input_context:\n",
    "        text += f'### Context:\\n{input_context}\\n\\n'\n",
    "\n",
    "    # Adding the response\n",
    "    text += f'### Response:\\n{response}'\n",
    "\n",
    "    return text\n",
    "\n",
    "# Applying the function to each data point and adding the resulting prompts as a new column\n",
    "text_column = [create_contextual_prompt(data_point) for data_point in data]\n",
    "data = data.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db911cf-88e9-4b8b-892a-9512201a97d1",
   "metadata": {},
   "source": [
    "## 10: Dataset Shuffling and Tokenization\n",
    "This step involves two key processes for dataset preparation: first, the dataset is shuffled to eliminate any order-based biases, using a fixed seed for consistent shuffling across different runs. Following this, the dataset undergoes tokenization, where a tokenizer is applied to each data point's 'prompt' field in batches, ensuring efficient data processing suitable for machine learning model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8749e-2236-4a3b-b40c-b504e2949505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset to ensure that the order of data points does not introduce any bias.\n",
    "# A fixed seed (1234 in this case) is used for reproducibility, ensuring the shuffle order remains consistent across runs.\n",
    "# data = data.shuffle(seed=1234)\n",
    "\n",
    "data = data.shuffle(seed=16)\n",
    "# Apply the tokenizer to each data point in the dataset.\n",
    "# The 'map' function applies the given lambda function to each element of the dataset.\n",
    "# 'base_tokenizer(samples[\"prompt\"])' tokenizes the 'prompt' field of each sample in the dataset.\n",
    "# 'batched=True' indicates that the tokenization should be done in batches for efficiency.\n",
    "data = data.map(lambda samples: base_tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6f15f-a41f-4212-aa63-7b0ec646974a",
   "metadata": {},
   "source": [
    "## 11: Splitting the Dataset into Training and Testing Sets\n",
    "This step involves dividing the dataset into training and testing subsets. The `train_test_split` method is used, allocating 10% of the data for testing (`test_size=0.1`) and the remainder for training. The subsets are then separately extracted for their respective purposes, with `train_data` representing the training set and `test_data` the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23fb0e-efc0-4952-8da6-615f8c935975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets.\n",
    "# The 'train_test_split' method divides the data, allocating 10% for testing and the rest for training.\n",
    "# The 'test_size=0.1' parameter specifies that 10% of the dataset should be used for the test set.\n",
    "data = data.train_test_split(test_size=0.8)\n",
    "\n",
    "# Extract the training data subset from the split.\n",
    "# The 'train' key accesses the portion of the dataset designated for training purposes.\n",
    "train_data = data[\"train\"]\n",
    "\n",
    "# Extract the testing data subset from the split.\n",
    "# The 'test' key accesses the portion of the dataset designated for testing purposes.\n",
    "test_data = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e904f94-80ef-4130-bf9b-8e6699aa84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995233b5-c469-49e5-9656-c05348332a8a",
   "metadata": {},
   "source": [
    "## 12: Configuring the Model for k-bit Training\n",
    "This step involves preparing the base model for `k-bit` training. It starts with enabling gradient checkpointing in the model, a technique that reduces memory usage by storing only some intermediate activations. This is beneficial for handling larger models or increasing batch sizes. The model is then passed through the `prepare_model_for_kbit_training` function, which likely configures it for training with reduced precision (k-bit). This configuration aims to improve efficiency in terms of memory usage and computational speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3bf0c-ae44-4d0c-abc4-f1670c7b5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for preparing a model for k-bit training.\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing for the base model.\n",
    "# Gradient checkpointing is a technique to reduce memory usage during training by saving only a subset of the intermediate activations.\n",
    "# This allows for training larger models or using larger batch sizes.\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the base model for k-bit training.\n",
    "# The 'prepare_model_for_kbit_training' function presumably configures the model for training with reduced precision (k-bit),\n",
    "# which can lead to efficiency improvements in both memory usage and computational speed.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Print the model configuration.\n",
    "print(base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e133dad-dcb4-472e-b76c-193314cf46ef",
   "metadata": {},
   "source": [
    "## 13: Identifying Linear Layers in a Model\n",
    "This code defines a function, `get_linear_layer_names`, which is designed to identify and list the names of linear layers in a given neural network model, based on a specified bit precision. It uses the `bitsandbytes` library to determine the class type of the linear layers (either 4-bit, 8-bit, or standard precision) and then iterates through all modules in the model to find instances of these layers. The function returns a list of unique names of the linear layers, excluding the 'lm_head' module typically associated with 16-bit precision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058491cf-6da9-4a4a-9861-60f5d83a7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def get_linear_layer_names(model, bit_precision=4):\n",
    "    \"\"\"\n",
    "    Identifies and returns the names of linear layers in the model based on the specified bit precision.\n",
    "\n",
    "    :param model: The neural network model to be inspected.\n",
    "    :param bit_precision: The bit precision of the linear layers to find (default: 4).\n",
    "    :return: A list of names of the linear layers in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the class type for the linear layer based on bit precision.\n",
    "    linear_class = bnb.nn.Linear4bit if bit_precision == 4 else bnb.nn.Linear8bitLt if bit_precision == 8 else torch.nn.Linear\n",
    "\n",
    "    linear_layer_names = set()  # Set to store unique names of linear layers.\n",
    "\n",
    "    # Iterate through all modules in the model to find instances of the specified linear layer class.\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, linear_class):\n",
    "            # Extract the base or last segment of the module name and add it to the set.\n",
    "            module_name_segment = name.split('.')[0 if name.count('.') == 0 else -1]\n",
    "            linear_layer_names.add(module_name_segment)\n",
    "\n",
    "    # Remove 'lm_head' from the set if present, typically applicable for 16-bit precision models.\n",
    "    linear_layer_names.discard('lm_head')\n",
    "\n",
    "    return list(linear_layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29616afa-b38e-4a8c-958f-9ead1c349fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = get_linear_layer_names(base_model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b6115-677d-4404-8450-7a2f41e7150f",
   "metadata": {},
   "source": [
    "## 14: Configuring and Applying LORA to the Base Model\n",
    "This step involves configuring and applying Low-Rank Adaptation (LORA) to the base model using the `peft` package. A `LoraConfig` object is created, specifying various parameters such as the rank of LORA layers (`r`), a scaling factor (`lora_alpha`), target modules for adaptation, dropout rate for regularization (`lora_dropout`), bias settings, and the task type (Causal Language Modeling). Then, using the `get_peft_model` function, these LORA adaptations are applied to the specified target modules in the base model, enhancing it for Parameter-Efficient Fine-Tuning (PEFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e8ce3-d94d-4387-b634-6c1920cbfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model  # Importing LoraConfig and get_peft_model from the 'peft' package.\n",
    "\n",
    "# Create a configuration object for LORA (Low-Rank Adaptation) layers.\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 'r' is the rank of the LORA layers, affecting the amount of parameters added.\n",
    "    lora_alpha=32,  # 'lora_alpha' is a scaling factor for LORA's low-rank matrices.\n",
    "    target_modules=modules,  # 'target_modules' are the modules in the model to be adapted by LORA.\n",
    "    lora_dropout=0.05,  # 'lora_dropout' specifies the dropout rate in LORA layers for regularization.\n",
    "    bias=\"none\",  # 'bias' determines the usage of bias in LORA layers, here set to 'none'.\n",
    "    task_type=\"CAUSAL_LM\"  # 'task_type' specifies the type of task, here set to causal language modeling.\n",
    ")\n",
    "\n",
    "# Enhance the base model with PEFT (Parameter-Efficient Fine-Tuning) using the specified LORA configuration.\n",
    "# This process involves applying the LORA adaptations to the specified target modules in the model.\n",
    "base_model = get_peft_model(base_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e49ed-875f-4b25-a6d6-4480c4f40b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of trainable and total parameters in the base model.\n",
    "# trainable, total = base_model.get_nb_trainable_parameters()\n",
    "\n",
    "# Print out the number of trainable parameters, total parameters, \n",
    "# and the percentage of parameters that are trainable.\n",
    "# This provides insight into the proportion of the model that can be updated during training.\n",
    "# print(f\"Trainable: {trainable} | Total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31f444-83ff-41e0-8b07-124b4d532268",
   "metadata": {},
   "source": [
    "## 15: Initializing the Supervised Fine-Tuning Trainer\n",
    "This step involves initializing the Supervised Fine-Tuning (SFT) Trainer for a transformer model. The process includes configuring the tokenizer, clearing the GPU cache for efficient memory usage, and setting up the trainer with specific parameters for model training and evaluation, PEFT configuration using LORA, training arguments like batch size, learning rate, and logging settings. The setup is tailored for language model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1ea29-e992-4173-80ee-f0b8f83a6e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers  # Importing the transformers library, which provides tools for working with transformer models.\n",
    "\n",
    "from trl import SFTTrainer  # Importing SFTTrainer from the trl (transformer reinforcement learning) package.\n",
    "# from transformers import TrainerArgument\n",
    "# from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
    "\n",
    "# Setting the padding token of the tokenizer to be the same as its end-of-sentence token.\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# Clearing the GPU cache to free up memory and avoid potential out-of-memory issues.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initializing the Supervised Fine-Tuning (SFT) Trainer.\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,  # The model to be fine-tuned.\n",
    "    train_dataset=train_data,  # The dataset for training.\n",
    "    eval_dataset=test_data,  # The dataset for evaluation.\n",
    "    dataset_text_field=\"prompt\",  # The field in the dataset that contains the text to be processed.\n",
    "    peft_config=lora_config,  # The PEFT (Parameter-Efficient Fine-Tuning) configuration, here using LORA.\n",
    "    args=transformers.TrainingArguments(  # Configuration for the training process.\n",
    "        per_device_train_batch_size=10,  # Batch size per device.\n",
    "        gradient_accumulation_steps=16,  # Number of steps to accumulate gradients before updating model weights.\n",
    "        warmup_steps=50,  # Absolute number of warmup steps for the learning rate scheduler.\n",
    "        max_steps=-1,  # Maximum number of training steps, -1 means unlimited.\n",
    "        learning_rate=1e-5,  # The learning rate for optimization.\n",
    "        logging_dir=\"./logs\",  # Directory where training logs will be stored.\n",
    "        logging_first_step=True,  # Log the first training step, useful for debugging.\n",
    "        logging_steps=20,  # Frequency of logging training information.\n",
    "        evaluation_strategy=\"steps\",  # Strategy to perform model evaluation.\n",
    "        optim=\"adamw_torch\",  # The optimizer to be used.\n",
    "        eval_steps=50,  # Number of steps before performing evaluation.\n",
    "        output_dir=\"/opt/app-root/src/data/v8-finance-3/outputs\",  # Directory to store output files.\n",
    "        load_best_model_at_end=True,  # Whether to load the best model at the end of training.\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_tokenizer, mlm=False),  # Data collator for language modeling.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc80bdb-f837-4283-ad93-b28346a47c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable caching in the model's configuration. \n",
    "# This is typically done during training to save memory, as caching activations is not necessary.\n",
    "# However, for inference, caching should be re-enabled for improved performance.\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# Start the training process using the SFTTrainer instance.\n",
    "# The function argument specifies the path to resume training from a specific checkpoint.\n",
    "# trainer.train(\"/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000\")\n",
    "# Initial Training without checkpoint\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721b55f-fcd2-4997-a7f8-a224d6158fc4",
   "metadata": {},
   "source": [
    "## 16: Publishing Model and Tokenizer to Hugging Face Model Hub\n",
    "This step involves uploading the fine-tuned model and its associated tokenizer to the Hugging Face Model Hub. The model and tokenizer are pushed to the hub under the repository name `\"Llama-2-7b-hf_finetuned_finance_jupyter\"`, making them accessible online for public use and download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6772e-ade0-472e-b76e-744f43d0787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the fine-tuned model to the Hugging Face Model Hub.\n",
    "# This makes the model available online for others to use and download.\n",
    "# \"Llama-2-7b-hf_finetuned_finance_jupyter_v7\" is the repository name on the Model Hub.\n",
    "base_model.push_to_hub(\"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\")\n",
    "\n",
    "# Similarly, push the tokenizer associated with the fine-tuned model to the Hugging Face Model Hub.\n",
    "# This ensures that users who download the model also have access to the correct tokenizer.\n",
    "# The repository name is kept the same for consistency and easy association with the model.\n",
    "base_tokenizer.push_to_hub(\"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74925f9-3f7e-4e79-a232-e5eb1b02fd0f",
   "metadata": {},
   "source": [
    "## 17: Loading and Utilizing a PEFT Enhanced Language Model\n",
    "The code involves loading a fine-tuned language model enhanced with Parameter-Efficient Fine-Tuning (PEFT) and its tokenizer from the Hugging Face Model Hub, using a specified identifier. The PEFT-enhanced model is loaded with settings for efficiency, including 4-bit loading and automatic device placement. After loading, the model and tokenizer are used to generate a response to a sample query, showcasing the practical application of the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ef37c-1b3e-4f7d-b3f3-734816a3a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig  # Import PEFT (Parameter-Efficient Fine-Tuning) related classes.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Importing necessary classes from transformers.\n",
    "\n",
    "# Identifier for the fine-tuned PEFT model on the Hugging Face Model Hub.\n",
    "peft_model_id = \"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\"\n",
    "\n",
    "# Load the PEFT configuration from the Hugging Face Model Hub using the model identifier.\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base causal language model specified in the PEFT config, enabling 4-bit loading for efficiency.\n",
    "# 'device_map=\"auto\"' automatically places the model on the most appropriate device (CPU/GPU).\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map='auto')\n",
    "\n",
    "# Load the tokenizer corresponding to the base model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the PEFT model from the pretrained model and config, enabling the use of PEFT enhancements.\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ad9d2-6313-4ee1-87c4-7d5e73aa0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_model_response(task_query=\"Will capital gains affect my tax bracket?\", inference_model=model, sequence_tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013553a-b5fe-4f48-831d-1de0afd26079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's load the JSON data from the file\n",
    "file_path =  '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'\n",
    "\n",
    "\n",
    "# Load the trainer state data\n",
    "with open(file_path, 'r') as file:\n",
    "    trainer_state = json.load(file)\n",
    "\n",
    "# Extract the metrics, ensuring we only take steps that have both a loss and learning rate\n",
    "steps_with_loss = [log_entry['step'] for log_entry in trainer_state['log_history'] if 'loss' in log_entry]\n",
    "losses = [log_entry['loss'] for log_entry in trainer_state['log_history'] if 'loss' in log_entry]\n",
    "\n",
    "steps_with_lr = [log_entry['step'] for log_entry in trainer_state['log_history'] if 'learning_rate' in log_entry]\n",
    "learning_rates = [log_entry['learning_rate'] for log_entry in trainer_state['log_history'] if 'learning_rate' in log_entry]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Loss vs. Steps with loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps_with_loss, losses, label='Loss', color='red')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Learning Rate vs. Steps with learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps_with_lr, learning_rates, label='Learning Rate', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7d3bb-80a7-4d48-b329-06af994f887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "file_path = '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    trainer_state_data = json.load(file)\n",
    "\n",
    "# Normalize the nested 'log_history' data into a flat table\n",
    "df = pd.json_normalize(trainer_state_data, record_path=['log_history'])\n",
    "\n",
    "# Filter out the required columns\n",
    "filtered_df = df[['epoch', 'learning_rate', 'loss', 'step']].dropna()\n",
    "\n",
    "# Save the filtered data as a CSV file\n",
    "csv_file_path = '/opt/app-root/src/data/training_data.csv'\n",
    "filtered_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea303-37c7-409f-9361-f73a29713329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
