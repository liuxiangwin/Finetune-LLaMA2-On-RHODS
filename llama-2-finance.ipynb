{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f78bf98-a0d9-4b7c-976c-1b8f7e0e6e88",
   "metadata": {},
   "source": [
    "## 1: Create model response function\n",
    "The create_model_response function is designed to generate a textual response from a given inference model based on a user's query. It formats the query into a template that the model understands, encodes this input, and then decodes the model's output into a human-readable response.\n",
    "\n",
    "### Parameters\n",
    "- `task_query (str)`: The user's query or task description that needs to be responded to.\n",
    "- `inference_model`: The machine learning model that generates the response. This should be pre-loaded and ready for inference.\n",
    "- `sequence_tokenizer`: A tokenizer that corresponds to the `inference_model`, used for both encoding the input and decoding the output.\n",
    "\n",
    "### Returns\n",
    "`str`: A string representing the model's response to the `task_query`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207c99e-6405-449c-9fcf-2973e646a8d7",
   "metadata": {},
   "source": [
    "## 2: Install Packages\n",
    "This script is a sequence of commands to install and update various Python packages, primarily focused on machine learning and data handling, using pip, the Python package installer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2744e0be-3f03-4882-9526-b8fc6c3cfa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.9/site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-24.0\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: torch in /opt/app-root/lib/python3.9/site-packages (from bitsandbytes) (2.0.1+cu118)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch->bitsandbytes) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch->bitsandbytes) (3.28.1)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch->bitsandbytes) (17.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch->bitsandbytes) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m205.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.0\n",
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-h2t24_mj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-h2t24_mj\n",
      "  Resolved https://github.com/huggingface/peft.git to commit a18734d87aa9ae6b94b5bdde192b265bfad7c0b3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (2.0.1+cu118)\n",
      "Collecting transformers (from peft==0.9.1.dev0)\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (from peft==0.9.1.dev0) (4.66.1)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.9.1.dev0)\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors (from peft==0.9.1.dev0)\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.9.1.dev0)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.9.1.dev0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.9.1.dev0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.9.1.dev0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.9.1.dev0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.9.1.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.9.1.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.9.1.dev0) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.9.1.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.9.1.dev0) (3.28.1)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.9.1.dev0) (17.0.6)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft==0.9.1.dev0)\n",
      "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m213.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->peft==0.9.1.dev0)\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.9.1.dev0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.1.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.1.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.1.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.1.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.9.1.dev0) (1.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m153.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.4/773.4 kB\u001b[0m \u001b[31m289.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m161.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.9.1.dev0-py3-none-any.whl size=194789 sha256=3369d75c460cc2ff8302f16b297aad3f591372bfa9c6a13f79a05925493c26aa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_itzofik/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n",
      "Successfully built peft\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed accelerate-0.28.0 huggingface-hub-0.21.4 peft-0.9.1.dev0 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "Collecting git+https://github.com/huggingface/accelerate.git\n",
      "  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-fx80nque\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-fx80nque\n",
      "  Resolved https://github.com/huggingface/accelerate.git to commit e8aaee5d9b9333979546f13e245e34380010fa54\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from accelerate==0.29.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.29.0.dev0) (3.28.1)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.29.0.dev0) (17.0.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.29.0.dev0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.29.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.29.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.0.dev0) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.29.0.dev0-py3-none-any.whl size=290745 sha256=369b8092317463d71d03ee21b3d4b99480dbe91ccc294e7d4ae9b3644e105997\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qrmpfd5k/wheels/60/53/1d/f8f7d9ed24f2b70cf9b37ecd31318a274049263effcc4b5bf3\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.28.0\n",
      "    Uninstalling accelerate-0.28.0:\n",
      "      Successfully uninstalled accelerate-0.28.0\n",
      "Successfully installed accelerate-0.29.0.dev0\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from datasets) (1.24.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/app-root/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m245.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m232.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m240.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m270.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/app-root/lib/python3.9/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/lib/python3.9/site-packages (3.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/app-root/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib/python3.9/site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/app-root/lib/python3.9/site-packages (from scipy) (1.24.4)\n",
      "Requirement already satisfied: ipywidgets in /opt/app-root/lib/python3.9/site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/app-root/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/app-root/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/app-root/lib/python3.9/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/app-root/lib/python3.9/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/app-root/lib/python3.9/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: decorator in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/app-root/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/app-root/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/app-root/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/app-root/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Collecting transformers==4.36.0\n",
      "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers==4.36.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.36.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.36.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.36.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers==4.36.0) (2023.11.17)\n",
      "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.2\n",
      "    Uninstalling transformers-4.38.2:\n",
      "      Successfully uninstalled transformers-4.38.2\n",
      "Successfully installed transformers-4.36.0\n",
      "Collecting trl==0.7.4\n",
      "  Downloading trl-0.7.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/app-root/lib/python3.9/site-packages (from trl==0.7.4) (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /opt/app-root/lib/python3.9/site-packages (from trl==0.7.4) (4.36.0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/app-root/lib/python3.9/site-packages (from trl==0.7.4) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib/python3.9/site-packages (from trl==0.7.4) (0.29.0.dev0)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib/python3.9/site-packages (from trl==0.7.4) (2.18.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.7.4)\n",
      "  Downloading tyro-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/app-root/lib/python3.9/site-packages (from torch>=1.4.0->trl==0.7.4) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.4.0->trl==0.7.4) (3.28.1)\n",
      "Requirement already satisfied: lit in /opt/app-root/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.4.0->trl==0.7.4) (17.0.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers>=4.18.0->trl==0.7.4) (4.66.1)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.7.4)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/app-root/lib/python3.9/site-packages (from tyro>=0.5.11->trl==0.7.4) (12.6.0)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.4)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib/python3.9/site-packages (from accelerate->trl==0.7.4) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->trl==0.7.4) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets->trl==0.7.4) (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets->trl==0.7.4) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers>=4.18.0->trl==0.7.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers>=4.18.0->trl==0.7.4) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers>=4.18.0->trl==0.7.4) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers>=4.18.0->trl==0.7.4) (2023.11.17)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/app-root/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/app-root/lib/python3.9/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.4) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch>=1.4.0->trl==0.7.4) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->trl==0.7.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets->trl==0.7.4) (2023.3.post1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch>=1.4.0->trl==0.7.4) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.7.4) (1.16.0)\n",
      "Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.7.3-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m241.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: shtab, docstring-parser, tyro, trl\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring_parser 0.8.1\n",
      "    Uninstalling docstring_parser-0.8.1:\n",
      "      Successfully uninstalled docstring_parser-0.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "codeflare-torchx 0.6.0.dev1 requires docstring-parser==0.8.1, but you have docstring-parser 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 shtab-1.7.1 trl-0.7.4 tyro-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U bitsandbytes\n",
    "\n",
    "!pip install  -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install ipywidgets\n",
    "\n",
    "#!pip install trl\n",
    "#!pip install  -U git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "!pip install transformers==4.36.0\n",
    "!pip install trl==0.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436d5005-d6a7-4f26-b588-aa34d4d5c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_response(task_query: str, inference_model, sequence_tokenizer) -> str:\n",
    "    processing_device = \"cuda:0\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
    "\n",
    "    ### Instruction:\n",
    "    {user_task_query}\n",
    "\n",
    "    ### Completion:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the template with the user's query\n",
    "    task_prompt = prompt_template.format(user_task_query=task_query)\n",
    "\n",
    "    # Encoding the prompt for the model\n",
    "    encoded_input = sequence_tokenizer(task_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Sending the encoded input to the designated processing device\n",
    "    model_input_tensor = encoded_input.to(processing_device)\n",
    "\n",
    "    # Generating tokens from the model based on the input\n",
    "    generated_token_ids = inference_model.generate(\n",
    "        **model_input_tensor, \n",
    "        max_new_tokens=1000, \n",
    "        do_sample=True, \n",
    "        pad_token_id=sequence_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decoding the generated tokens to form the response\n",
    "    generated_response = sequence_tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_response[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31579ff4-ad82-48a2-bd87-f89b4e4a3198",
   "metadata": {},
   "source": [
    "### 3: System and CUDA Check\n",
    "This code helps you determine your PyTorch version and assess the availability of CUDA (GPU acceleration). If CUDA is accessible, it offers insights into your GPUs, including names, properties, and memory usage. In case CUDA is not available, it informs you that no GPU has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bb7ca9-729e-43cb-8308-b02bbdcf8397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA is available.\n",
      "Number of GPU: 3\n",
      "Current Device : <function current_device at 0x7f8aa0653940>\n",
      "Device 0: NVIDIA A10G\n",
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)\n",
      "Memory GB: 21.98 GB\n",
      "GPU Allocated: 0.00 GB\n",
      "GPU Cached:    0.00 GB\n",
      "Current Device : <function current_device at 0x7f8aa0653940>\n",
      "Device 1: NVIDIA A10G\n",
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)\n",
      "Memory GB: 21.98 GB\n",
      "GPU Allocated: 0.00 GB\n",
      "GPU Cached:    0.00 GB\n",
      "Current Device : <function current_device at 0x7f8aa0653940>\n",
      "Device 2: NVIDIA A10G\n",
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22502MB, multi_processor_count=80)\n",
      "Memory GB: 21.98 GB\n",
      "GPU Allocated: 0.00 GB\n",
      "GPU Cached:    0.00 GB\n",
      "<function _get_device_index at 0x7f8aa07d2790>\n",
      "<function current_device at 0x7f8aa0653940>\n",
      "1,2,3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"Number of GPU:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Current Device : {torch.cuda.current_device}\")\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(torch.cuda.get_device_properties(i))\n",
    "        print(f\"Memory GB: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"GPU Cached:    {torch.cuda.memory_reserved(i) / 1024 ** 3:.2f} GB\")\n",
    "        \n",
    "        \n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "\n",
    "print(torch.cuda._get_device_index)\n",
    "print(torch.cuda.current_device)\n",
    "\n",
    "#torch.cuda.set_device(3)\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "print(CUDA_VISIBLE_DEVICES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ccc12-d1b5-4bd9-920f-27cee044a335",
   "metadata": {},
   "source": [
    "This command, `!nvidia-smi`, provides a concise overview of NVIDIA GPU information, displaying details like GPU model, memory usage, and temperature in the current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76acbdc-a175-4d6d-bcbd-d6a6915155f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 01:32:40 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   17C    P0             40W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A10G                    On  |   00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   17C    P0             41W /  300W |       3MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A10G                    On  |   00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   17C    P0             22W /  300W |       3MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   17C    P0             21W /  300W |       3MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7ff83-25eb-480b-9972-686a1bdf33cf",
   "metadata": {},
   "source": [
    "## 4: Load and Display Dataset\n",
    "This section of the code leverages the Hugging Face datasets library to import a financial dataset from Alpaca. It then transforms this dataset into a Pandas DataFrame for more flexible data handling. To ensure comprehensive visibility, the display settings of Pandas are tweaked to show all columns and the entire content of each cell. The code culminates by showcasing the first 10 rows of the DataFrame, offering a preliminary glimpse into the dataset's structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a308d105-58ae-43ac-9ee4-6439ccaebc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>For a car, what scams can be plotted with 0% financing vs rebate?</td>\n",
       "      <td></td>\n",
       "      <td>The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>Why does it matter if a Central Bank has a negative rather than 0% interest rate?</td>\n",
       "      <td></td>\n",
       "      <td>That is kind of the point, one of the hopes is that it incentivizes banks to stop storing money and start injecting it into the economy themselves. Compared to the European Central Bank investing directly into the economy the way the US central bank has been doing. (The Federal Reserve buying mortgage backed securities) On a country level, individual European countries have tried this before in recent times with no noticeable effect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Where should I be investing my money?</td>\n",
       "      <td></td>\n",
       "      <td>Pay off your debt.  As you witnessed, no \"investment\" % is guaranteed.  But your debt payments are... so if you have cash, the best way to \"invest\" it is to pay off your debt.  Since your car is depreciating while your house may be appreciating (don't know but it's possible) you should pay off your car loan first.  You're losing money in more than one way on that investment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>Specifically when do options expire?</td>\n",
       "      <td></td>\n",
       "      <td>Equity options, at least those traded in the American exchanges, actually expire the Saturday after the 3rd Friday of the month.  However, the choice to trade or exercise the options must be specified by the 3rd Friday. This is outlined by the CBOE, who oversees the exchange of equity options.  Their FAQ regarding option expiration can be found at http://www.cboe.com/LearnCenter/Concepts/Beyond/expiration.aspx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Negative Balance from Automatic Options Exercise. What to do?</td>\n",
       "      <td></td>\n",
       "      <td>Automatic exercisions can be extremely risky, and the closer to the money the options are, the riskier their exercisions are. It is unlikely that the entire account has negative equity since a responsible broker would forcibly close all positions and pursue the holder for the balance of the debt to reduce solvency risk.  Since the broker has automatically exercised a near the money option, it's solvency policy is already risky. Regardless of whether there is negative equity or simply a liability, the least risky course of action is to sell enough of the underlying to satisfy the loan by closing all other positions if necessary as soon as possible. If there is a negative equity after trying to satisfy the loan, the account will need to be funded for the balance of the loan to pay for purchases of the underlying to fully satisfy the loan. Since the underlying can move in such a way to cause this loan to increase, the account should also be funded as soon as possible if necessary. Accounts after exercise For deep in the money exercised options, a call turns into a long underlying on margin while a put turns into a short underlying. The next decision should be based upon risk and position selection.  First, if the position is no longer attractive, it should be closed.  Since it's deep in the money, simply closing out the exposure to the underlying should extinguish the liability as cash is not marginable, so the cash received from the closing out of the position will repay any margin debt. If the position in the underlying is still attractive then the liability should be managed according to one's liability policy and of course to margin limits. In a margin account, closing the underlying positions on the same day as the exercise will only be considered a day trade.  If the positions are closed on any business day after the exercision, there will be no penalty or restriction. Cash option accounts While this is possible, many brokers force an upgrade to a margin account, and the ShareBuilder Options Account Agreement seems ambiguous, but their options trading page implies the upgrade. In a cash account, equities are not marginable, so any margin will trigger a margin call.  If the margin debt did not trigger a margin call then it is unlikely that it is a cash account as margin for any security in a cash account except for certain options trades is 100%. Equities are convertible to cash presumably at the bid, so during a call exercise, the exercisor or exercisor's broker pays cash for the underlying at the exercise price, and any deficit is financed with debt, thus underlying can be sold to satisfy that debt or be sold for cash as one normally would. To preempt a forced exercise as a call holder, one could short the underlying, but this will be more expensive, and since probably no broker allows shorting against the box because of its intended use to circumvent capital gains taxes by fraud.  The least expensive way to trade out of options positions is to close them themselves rather than take delivery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>Approximation of equity value for company in default</td>\n",
       "      <td></td>\n",
       "      <td>Generally \"default\" means that the company cannot pay off their debts, and since debt holders get paid before equity holders, their equity would be effectively worthless. That said, companies can emerge from Chapter 11 bankruptcy (reorganization) and retain equity value, but it is rare. Most times, stocks are de-listed or frozen on stock exchanges, and company's reorganization plan will cancel all existing equity shares, instead focusing all of their attention on paying back as much debt as possible. If the company issues new equity after reorganizing, it might provide a way for holders of the original equity to exchange their shares for the new equity, but it is rare, and the value is usually significantly less that the value of the original equity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>Is it true that 90% of investors lose their money?</td>\n",
       "      <td></td>\n",
       "      <td>The game is not zero sum. When a friend and I chop down a tree, and build a house from it, the house has value, far greater than the value of a standing tree. Our labor has turned into something of value.  In theory, a company starts from an idea, and offers either a good or service to create value. There are scams that make it seem like a Vegas casino. There are times a stock will trade for well above what it should. When I buy the S&amp;P index at a fair price for 1000 (through an etf or fund) and years later it's 1400, the gain isn't out of someone else's pocket, else the amount of wealth in the world would be fixed and that's not the case. Over time, investors lag the market return for multiple reasons, trading costs, bad timing, etc. Statements such as \"90% lose money\" are hyperbole meant to separate you from your money. A self fulfilling prophesy.   The question of lagging the market is another story - I have no data to support my observation, but I'd imagine that well over 90% lag the broad market. A detailed explanation is too long for this forum, but simply put, there are trading costs. If I invest in an S&amp;P ETF that costs .1% per year, I'll see a return of say 9.9% over decades if the market return is 10%. Over 40 years, this is 4364% compounded, vs the index 4526% compounded, a difference of less than 4% in final wealth. There are load funds that charge more than this just to buy in (5% anyone?).  Lagging by a small fraction is a far cry from 'losing money.'  There is an annual report by a company named Dalbar that tracks investor performance. For the 20 year period ending 12/31/10 the S&amp;P returned 9.14% and Dalbar calculates the average investor had an average return of 3.83%. Pretty bad, but not zero. Since you don't cite a particular article or source, there may be more to the story. Day traders are likely to lose. As are a series of other types of traders in other markets, Forex for one.  While your question may be interesting, its premise of \"many experts say....\" without naming even one leaves room for doubt.  Note - I've updated the link for the 2015 report. And 4 years later, I see that when searching on that 90% statistic, the articles are about day traders. That actually makes sense to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>Can a company charge you for services never requested or received?</td>\n",
       "      <td></td>\n",
       "      <td>In general, you can only be charged for services if there is some kind of contract. The contract doesn't have to be written, but you have to have agreed to it somehow. However, it is possible that you entered into a contract due to some clause in the home purchase contract or the contract with the home owners' association. There are also sometimes services you are legally required to get, such as regular inspection of heating furnaces (though I don't think this translates to automatic contracts). But in any case you would not be liable for services rendered before you entered into the contract, which sounds like it's the case here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Working out if I should be registered as self-employed in the UK</td>\n",
       "      <td></td>\n",
       "      <td>Being self employed just means you fill out some more forms in your annual self assessment for your \"profit\" from being self employed.  Profit = all the money you receive, minus any tax deductible cost that you spent for making that money (and all the cost must be documented, which means you have a folder with all the receipts and keep it safe). You pay normal income tax on all the profit, which means it is just added to your taxable income. What you do with the profit is up to you; you don't pay yourself a salary, just take the money (make sure you leave enough to pay your taxes).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>About eToro investments</td>\n",
       "      <td></td>\n",
       "      <td>For eToro, just like any other brokerage firm, you can lose your entire capital. I suggest that you invest in one or more exchange-traded funds that track major indexes.  If not, just put your money in fixed deposit accounts; gain a bit of interest and establish an emergency fund first before investing money that you feel you are able to lose.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text  \\\n",
       "0        \n",
       "1        \n",
       "2        \n",
       "3        \n",
       "4        \n",
       "5        \n",
       "6        \n",
       "7        \n",
       "8        \n",
       "9        \n",
       "\n",
       "                                                                         instruction  \\\n",
       "0                  For a car, what scams can be plotted with 0% financing vs rebate?   \n",
       "1  Why does it matter if a Central Bank has a negative rather than 0% interest rate?   \n",
       "2                                              Where should I be investing my money?   \n",
       "3                                               Specifically when do options expire?   \n",
       "4                      Negative Balance from Automatic Options Exercise. What to do?   \n",
       "5                               Approximation of equity value for company in default   \n",
       "6                                 Is it true that 90% of investors lose their money?   \n",
       "7                 Can a company charge you for services never requested or received?   \n",
       "8                   Working out if I should be registered as self-employed in the UK   \n",
       "9                                                            About eToro investments   \n",
       "\n",
       "  input  \\\n",
       "0         \n",
       "1         \n",
       "2         \n",
       "3         \n",
       "4         \n",
       "5         \n",
       "6         \n",
       "7         \n",
       "8         \n",
       "9         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        output  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        That is kind of the point, one of the hopes is that it incentivizes banks to stop storing money and start injecting it into the economy themselves. Compared to the European Central Bank investing directly into the economy the way the US central bank has been doing. (The Federal Reserve buying mortgage backed securities) On a country level, individual European countries have tried this before in recent times with no noticeable effect.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Pay off your debt.  As you witnessed, no \"investment\" % is guaranteed.  But your debt payments are... so if you have cash, the best way to \"invest\" it is to pay off your debt.  Since your car is depreciating while your house may be appreciating (don't know but it's possible) you should pay off your car loan first.  You're losing money in more than one way on that investment.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Equity options, at least those traded in the American exchanges, actually expire the Saturday after the 3rd Friday of the month.  However, the choice to trade or exercise the options must be specified by the 3rd Friday. This is outlined by the CBOE, who oversees the exchange of equity options.  Their FAQ regarding option expiration can be found at http://www.cboe.com/LearnCenter/Concepts/Beyond/expiration.aspx.  \n",
       "4  Automatic exercisions can be extremely risky, and the closer to the money the options are, the riskier their exercisions are. It is unlikely that the entire account has negative equity since a responsible broker would forcibly close all positions and pursue the holder for the balance of the debt to reduce solvency risk.  Since the broker has automatically exercised a near the money option, it's solvency policy is already risky. Regardless of whether there is negative equity or simply a liability, the least risky course of action is to sell enough of the underlying to satisfy the loan by closing all other positions if necessary as soon as possible. If there is a negative equity after trying to satisfy the loan, the account will need to be funded for the balance of the loan to pay for purchases of the underlying to fully satisfy the loan. Since the underlying can move in such a way to cause this loan to increase, the account should also be funded as soon as possible if necessary. Accounts after exercise For deep in the money exercised options, a call turns into a long underlying on margin while a put turns into a short underlying. The next decision should be based upon risk and position selection.  First, if the position is no longer attractive, it should be closed.  Since it's deep in the money, simply closing out the exposure to the underlying should extinguish the liability as cash is not marginable, so the cash received from the closing out of the position will repay any margin debt. If the position in the underlying is still attractive then the liability should be managed according to one's liability policy and of course to margin limits. In a margin account, closing the underlying positions on the same day as the exercise will only be considered a day trade.  If the positions are closed on any business day after the exercision, there will be no penalty or restriction. Cash option accounts While this is possible, many brokers force an upgrade to a margin account, and the ShareBuilder Options Account Agreement seems ambiguous, but their options trading page implies the upgrade. In a cash account, equities are not marginable, so any margin will trigger a margin call.  If the margin debt did not trigger a margin call then it is unlikely that it is a cash account as margin for any security in a cash account except for certain options trades is 100%. Equities are convertible to cash presumably at the bid, so during a call exercise, the exercisor or exercisor's broker pays cash for the underlying at the exercise price, and any deficit is financed with debt, thus underlying can be sold to satisfy that debt or be sold for cash as one normally would. To preempt a forced exercise as a call holder, one could short the underlying, but this will be more expensive, and since probably no broker allows shorting against the box because of its intended use to circumvent capital gains taxes by fraud.  The least expensive way to trade out of options positions is to close them themselves rather than take delivery.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Generally \"default\" means that the company cannot pay off their debts, and since debt holders get paid before equity holders, their equity would be effectively worthless. That said, companies can emerge from Chapter 11 bankruptcy (reorganization) and retain equity value, but it is rare. Most times, stocks are de-listed or frozen on stock exchanges, and company's reorganization plan will cancel all existing equity shares, instead focusing all of their attention on paying back as much debt as possible. If the company issues new equity after reorganizing, it might provide a way for holders of the original equity to exchange their shares for the new equity, but it is rare, and the value is usually significantly less that the value of the original equity.  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The game is not zero sum. When a friend and I chop down a tree, and build a house from it, the house has value, far greater than the value of a standing tree. Our labor has turned into something of value.  In theory, a company starts from an idea, and offers either a good or service to create value. There are scams that make it seem like a Vegas casino. There are times a stock will trade for well above what it should. When I buy the S&P index at a fair price for 1000 (through an etf or fund) and years later it's 1400, the gain isn't out of someone else's pocket, else the amount of wealth in the world would be fixed and that's not the case. Over time, investors lag the market return for multiple reasons, trading costs, bad timing, etc. Statements such as \"90% lose money\" are hyperbole meant to separate you from your money. A self fulfilling prophesy.   The question of lagging the market is another story - I have no data to support my observation, but I'd imagine that well over 90% lag the broad market. A detailed explanation is too long for this forum, but simply put, there are trading costs. If I invest in an S&P ETF that costs .1% per year, I'll see a return of say 9.9% over decades if the market return is 10%. Over 40 years, this is 4364% compounded, vs the index 4526% compounded, a difference of less than 4% in final wealth. There are load funds that charge more than this just to buy in (5% anyone?).  Lagging by a small fraction is a far cry from 'losing money.'  There is an annual report by a company named Dalbar that tracks investor performance. For the 20 year period ending 12/31/10 the S&P returned 9.14% and Dalbar calculates the average investor had an average return of 3.83%. Pretty bad, but not zero. Since you don't cite a particular article or source, there may be more to the story. Day traders are likely to lose. As are a series of other types of traders in other markets, Forex for one.  While your question may be interesting, its premise of \"many experts say....\" without naming even one leaves room for doubt.  Note - I've updated the link for the 2015 report. And 4 years later, I see that when searching on that 90% statistic, the articles are about day traders. That actually makes sense to me.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In general, you can only be charged for services if there is some kind of contract. The contract doesn't have to be written, but you have to have agreed to it somehow. However, it is possible that you entered into a contract due to some clause in the home purchase contract or the contract with the home owners' association. There are also sometimes services you are legally required to get, such as regular inspection of heating furnaces (though I don't think this translates to automatic contracts). But in any case you would not be liable for services rendered before you entered into the contract, which sounds like it's the case here.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Being self employed just means you fill out some more forms in your annual self assessment for your \"profit\" from being self employed.  Profit = all the money you receive, minus any tax deductible cost that you spent for making that money (and all the cost must be documented, which means you have a folder with all the receipts and keep it safe). You pay normal income tax on all the profit, which means it is just added to your taxable income. What you do with the profit is up to you; you don't pay yourself a salary, just take the money (make sure you leave enough to pay your taxes).  \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    For eToro, just like any other brokerage firm, you can lose your entire capital. I suggest that you invest in one or more exchange-traded funds that track major indexes.  If not, just put your money in fixed deposit accounts; gain a bit of interest and establish an emergency fund first before investing money that you feel you are able to lose.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)  # Show all columns in the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure the full content of each cell is displayed\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset(\"gbharti/finance-alpaca\",\n",
    "                    # split='train')\n",
    "                    split=\"train[:50%]\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb27c76-8e47-4abc-8578-1c9aa9751926",
   "metadata": {},
   "source": [
    "## 5: Data Analysis and Visualization\n",
    "This code uses Pandas and Matplotlib to provide a comprehensive analysis of a DataFrame. It includes basic information such as the number of rows, columns, and column names, memory usage, data types, and descriptive statistics for character counts in specific columns. It also displays distributions and additional detailed statistics like missing values and unique values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1dae57-6c9c-45de-8c99-a07e285283f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Dataset Information:\n",
      "Number of Rows: 34456\n",
      "Number of Columns: 4\n",
      "Column Names: ['text', 'instruction', 'input', 'output']\n",
      "\n",
      "Memory Usage by Column:\n",
      "Index               128\n",
      "text            1963992\n",
      "instruction     4152488\n",
      "input           2430643\n",
      "output         25983181\n",
      "dtype: int64\n",
      "\n",
      "Data Types of Each Column:\n",
      "text           object\n",
      "instruction    object\n",
      "input          object\n",
      "output         object\n",
      "dtype: object\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Character Counts:\n",
      "       num_characters_instruction  num_characters_input  num_characters_output\n",
      "count                34456.000000          34456.000000           34456.000000\n",
      "mean                    60.300064             12.567506             646.934235\n",
      "std                     21.645898             51.977090             774.674485\n",
      "min                      9.000000              0.000000               0.000000\n",
      "25%                     45.000000              0.000000             145.000000\n",
      "50%                     57.000000              0.000000             428.000000\n",
      "75%                     72.000000              0.000000             831.000000\n",
      "max                    432.000000           2625.000000           16984.000000\n",
      "\n",
      "Missing Values in Each Column:\n",
      "text                          0\n",
      "instruction                   0\n",
      "input                         0\n",
      "output                        0\n",
      "num_characters_instruction    0\n",
      "num_characters_input          0\n",
      "num_characters_output         0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values in Each Column:\n",
      "text                              1\n",
      "instruction                   24028\n",
      "input                          6766\n",
      "output                        34163\n",
      "num_characters_instruction      177\n",
      "num_characters_input            419\n",
      "num_characters_output          3221\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and it's already loaded\n",
    "\n",
    "# Basic Dataset Information\n",
    "print(\"Basic Dataset Information:\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(f\"Column Names: {df.columns.tolist()}\", end=\"\\n\\n\")\n",
    "\n",
    "# Memory Usage\n",
    "print(\"Memory Usage by Column:\")\n",
    "print(df.memory_usage(deep=True), end=\"\\n\\n\")\n",
    "\n",
    "# Data Types\n",
    "print(\"Data Types of Each Column:\")\n",
    "print(df.dtypes, end=\"\\n\\n\")\n",
    "\n",
    "# Calculating the length of each cell in each column\n",
    "analysis_df = df.copy()\n",
    "analysis_df['num_characters_instruction'] = analysis_df['instruction'].apply(len)\n",
    "analysis_df['num_characters_input'] = analysis_df['input'].apply(len)\n",
    "analysis_df['num_characters_output'] = analysis_df['output'].apply(len)\n",
    "\n",
    "# Show Distribution\n",
    "analysis_df.hist(column=['num_characters_instruction', 'num_characters_input', 'num_characters_output'], bins=30, figsize=(12, 8))\n",
    "plt.suptitle('Distribution of Character Counts in Each Column')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive Statistics for Character Counts\n",
    "print(\"Descriptive Statistics for Character Counts:\")\n",
    "print(analysis_df[['num_characters_instruction', 'num_characters_input', 'num_characters_output']].describe(), end=\"\\n\\n\")\n",
    "\n",
    "# Additional Detailed Statistics\n",
    "max_chars_instruction = analysis_df['num_characters_instruction'].max()\n",
    "max_chars_input = analysis_df['num_characters_input'].max()\n",
    "max_chars_output = analysis_df['num_characters_output'].max()\n",
    "\n",
    "min_chars_instruction = analysis_df['num_characters_instruction'].min()\n",
    "min_chars_input = analysis_df['num_characters_input'].min()\n",
    "min_chars_output = analysis_df['num_characters_output'].min()\n",
    "\n",
    "# Print detailed statistics\n",
    "# Missing Values\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(analysis_df.isnull().sum(), end=\"\\n\\n\")\n",
    "\n",
    "# Unique Values\n",
    "print(\"Unique Values in Each Column:\")\n",
    "print(analysis_df.nunique(), end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b91511-9a58-499c-be96-4d4032e6a085",
   "metadata": {},
   "source": [
    "## 6: Configure BitsAndBytes for Efficient Model Inference\n",
    "This code configures the `BitsAndBytes` feature in Hugging Face Transformers using `BitsAndBytesConfig`, enhancing model efficiency. The setup includes enabling 4-bit model loading, activating double quantization, setting the quantization type to `'nf4'`, and specifying `torch.bfloat16` as the compute data type. These configurations collectively aim to optimize memory usage and computational efficiency while balancing precision and speed for large model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac61500-8a23-44a5-ad87-5de1dd39ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig allows the configuration of the BitsAndBytes feature of Hugging Face Transformers.\n",
    "# This feature enables efficient model inference by reducing the model size and computational requirements.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enables loading the model in a 4-bit quantized format to reduce memory usage.\n",
    "    bnb_4bit_use_double_quant=True,  # Activates double quantization, which quantizes not just the weights but also the activations.\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Sets the quantization type to 'nf4', a 4-bit number format for quantization.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Specifies bfloat16 as the data type for computation, balancing precision and speed.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa7932-9c0c-46fb-b72f-ef4ea3e05fce",
   "metadata": {},
   "source": [
    "## 7: Load Pre-Trained Model and Tokenizer\n",
    "This segment involves loading a pre-trained causal language model and its associated tokenizer from Hugging Face's model repository. The specific model loaded is identified as `\"meta-llama/Llama-2-7b-hf\"`. The model is configured for enhanced efficiency using the previously defined `BitsAndBytes` configuration. Additionally, the `device_map=\"auto\"` setting is used to automatically place the model on the most suitable computing device (CPU or GPU). For input text processing, the corresponding tokenizer is loaded with an added end-of-sentence token, ensuring proper tokenization of input sequences for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d35770-2d9e-4c8a-8f6f-a0fda446bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "token = \"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\"\n",
    "HfFolder.save_token(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7aae13-f176-4b35-8f97-459c38e359ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0621a8f55249bcbe59c8158bb3a3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is the identifier for the model we want to load from Hugging Face's model repository.\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load the pre-trained causal language model from Hugging Face with the specified model ID.\n",
    "# The model is configured for quantization using the previously defined BitsAndBytesConfig to improve efficiency.\n",
    "# 'device_map=\"auto\"' allows the model to be placed on the most appropriate device (CPU/GPU) automatically.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                  quantization_config=bnb_config, \n",
    "                                                  device_map=\"auto\")\n",
    "\n",
    "# Load the tokenizer associated with the pre-trained model.\n",
    "# The tokenizer is responsible for converting input text into a format that the model can understand (tokens).\n",
    "# 'add_eos_token=True' ensures that the end-of-sentence token is appended to the input sequences.\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f136463-d922-4c49-b9ac-1a24592aa3e3",
   "metadata": {},
   "source": [
    "## 8: Generate and Display Model Response\n",
    "This code generates a language model completion for the given query using the previously initialized base_model and base_tokenizer. The result is then printed, showcasing the model's response to the input question about capital gains and tax brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b153a46a-4104-4efe-81a8-d4c81c98aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
      "\n",
      "    ### Instruction:\n",
      "    Will capital gains affect my tax bracket?\n",
      "\n",
      "    ### Completion:\n",
      "     февраль 2019 г.\n",
      "\n",
      "\n",
      "### Что происходит в текущей версии\n",
      "\n",
      "- [x] Добавлена возможность отправки сообщений\n",
      "- [x] Добавлена возможность отправки файлов\n",
      "- [x] Добавлена возможность отправки фотографий\n",
      "- [x] Добавлена возможность отправки видео\n",
      "- [x] Добавлена возможность отправки аудио\n",
      "- [x] Добавлена возможность отправки файла\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватаркой\n",
      "- [x] Добавлена возможность отправки файла с фотографией\n",
      "- [x] Добавлена возможность отправки файла с аудио\n",
      "- [x] Добавлена возможность отправки файла с видео\n",
      "- [x] Добавлена возможность отправки файла с аватар\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model for a specified query using the 'create_model_response' function.\n",
    "# The query here is \"Will capital gains affect my tax bracket?\"\n",
    "# 'base_model' is the loaded language model and 'base_tokenizer' is the corresponding tokenizer.\n",
    "# The function will process the query, generate a response using the model, and return the result as a string.\n",
    "result = create_model_response(task_query=\"Will capital gains affect my tax bracket?\",\n",
    "                               inference_model=base_model,\n",
    "                               sequence_tokenizer=base_tokenizer)\n",
    "\n",
    "# Print the result to display the model-generated response to the query.\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecd834-1f7f-4965-ab3e-2c9f812bd3b1",
   "metadata": {},
   "source": [
    "## 9. Function: create_contextual_prompt\n",
    "This function constructs a formatted prompt for a given task. It takes a dictionary containing key information about the task - specifically, an instruction, optional input context, and an expected output or response.\n",
    "\n",
    "### Parameters\n",
    "- `data_point (dict):` A dictionary with at least two keys: instruction and output, and an optional input key. These keys correspond to:\n",
    "- `instruction:` The task's instructions.\n",
    "- `input:` Optional additional context or information relevant to the task.\n",
    "- `output:` The expected response or result for the task.\n",
    "\n",
    "### Returns\n",
    "- `str:` A string that represents the complete, formatted prompt. This prompt is structured into sections labeled 'Instruction', 'Context' (if available), and 'Response'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8abb82-66ef-4384-b412-9043b86a4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a textual prompt incorporating instructions, optional context, and a response.\n",
    "\n",
    "    :param data_point: A dictionary containing instruction, optional input context, and output.\n",
    "    :return: A string representing a formatted and contextualized prompt.\n",
    "    \"\"\"\n",
    "    instruction = data_point[\"instruction\"]\n",
    "    input_context = data_point.get(\"input\")\n",
    "    response = data_point[\"output\"]\n",
    "\n",
    "    # Base prompt structure\n",
    "    text = 'This is a task instruction. Complete the task as described.\\n\\n'\n",
    "\n",
    "    # Adding instruction\n",
    "    text += f'### Instruction:\\n{instruction}\\n\\n'\n",
    "\n",
    "    # Conditionally adding input context if available\n",
    "    if input_context:\n",
    "        text += f'### Context:\\n{input_context}\\n\\n'\n",
    "\n",
    "    # Adding the response\n",
    "    text += f'### Response:\\n{response}'\n",
    "\n",
    "    return text\n",
    "\n",
    "# Applying the function to each data point and adding the resulting prompts as a new column\n",
    "text_column = [create_contextual_prompt(data_point) for data_point in data]\n",
    "data = data.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db911cf-88e9-4b8b-892a-9512201a97d1",
   "metadata": {},
   "source": [
    "## 10: Dataset Shuffling and Tokenization\n",
    "This step involves two key processes for dataset preparation: first, the dataset is shuffled to eliminate any order-based biases, using a fixed seed for consistent shuffling across different runs. Following this, the dataset undergoes tokenization, where a tokenizer is applied to each data point's 'prompt' field in batches, ensuring efficient data processing suitable for machine learning model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec8749e-2236-4a3b-b40c-b504e2949505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dd7af870004f34be1d20e97ee170fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shuffle the dataset to ensure that the order of data points does not introduce any bias.\n",
    "# A fixed seed (1234 in this case) is used for reproducibility, ensuring the shuffle order remains consistent across runs.\n",
    "# data = data.shuffle(seed=1234)\n",
    "\n",
    "data = data.shuffle(seed=16)\n",
    "# Apply the tokenizer to each data point in the dataset.\n",
    "# The 'map' function applies the given lambda function to each element of the dataset.\n",
    "# 'base_tokenizer(samples[\"prompt\"])' tokenizes the 'prompt' field of each sample in the dataset.\n",
    "# 'batched=True' indicates that the tokenization should be done in batches for efficiency.\n",
    "data = data.map(lambda samples: base_tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6f15f-a41f-4212-aa63-7b0ec646974a",
   "metadata": {},
   "source": [
    "## 11: Splitting the Dataset into Training and Testing Sets\n",
    "This step involves dividing the dataset into training and testing subsets. The `train_test_split` method is used, allocating 10% of the data for testing (`test_size=0.1`) and the remainder for training. The subsets are then separately extracted for their respective purposes, with `train_data` representing the training set and `test_data` the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc23fb0e-efc0-4952-8da6-615f8c935975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets.\n",
    "# The 'train_test_split' method divides the data, allocating 10% for testing and the rest for training.\n",
    "# The 'test_size=0.1' parameter specifies that 10% of the dataset should be used for the test set.\n",
    "data = data.train_test_split(test_size=0.8)\n",
    "\n",
    "# Extract the training data subset from the split.\n",
    "# The 'train' key accesses the portion of the dataset designated for training purposes.\n",
    "train_data = data[\"train\"]\n",
    "\n",
    "# Extract the testing data subset from the split.\n",
    "# The 'test' key accesses the portion of the dataset designated for testing purposes.\n",
    "test_data = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e904f94-80ef-4130-bf9b-8e6699aa84b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'instruction', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6891\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'instruction', 'input', 'output', 'prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 27565\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995233b5-c469-49e5-9656-c05348332a8a",
   "metadata": {},
   "source": [
    "## 12: Configuring the Model for k-bit Training\n",
    "This step involves preparing the base model for `k-bit` training. It starts with enabling gradient checkpointing in the model, a technique that reduces memory usage by storing only some intermediate activations. This is beneficial for handling larger models or increasing batch sizes. The model is then passed through the `prepare_model_for_kbit_training` function, which likely configures it for training with reduced precision (k-bit). This configuration aims to improve efficiency in terms of memory usage and computational speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b3bf0c-ae44-4d0c-abc4-f1670c7b5770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the function for preparing a model for k-bit training.\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing for the base model.\n",
    "# Gradient checkpointing is a technique to reduce memory usage during training by saving only a subset of the intermediate activations.\n",
    "# This allows for training larger models or using larger batch sizes.\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare the base model for k-bit training.\n",
    "# The 'prepare_model_for_kbit_training' function presumably configures the model for training with reduced precision (k-bit),\n",
    "# which can lead to efficiency improvements in both memory usage and computational speed.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Print the model configuration.\n",
    "print(base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e133dad-dcb4-472e-b76c-193314cf46ef",
   "metadata": {},
   "source": [
    "## 13: Identifying Linear Layers in a Model\n",
    "This code defines a function, `get_linear_layer_names`, which is designed to identify and list the names of linear layers in a given neural network model, based on a specified bit precision. It uses the `bitsandbytes` library to determine the class type of the linear layers (either 4-bit, 8-bit, or standard precision) and then iterates through all modules in the model to find instances of these layers. The function returns a list of unique names of the linear layers, excluding the 'lm_head' module typically associated with 16-bit precision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "058491cf-6da9-4a4a-9861-60f5d83a7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def get_linear_layer_names(model, bit_precision=4):\n",
    "    \"\"\"\n",
    "    Identifies and returns the names of linear layers in the model based on the specified bit precision.\n",
    "\n",
    "    :param model: The neural network model to be inspected.\n",
    "    :param bit_precision: The bit precision of the linear layers to find (default: 4).\n",
    "    :return: A list of names of the linear layers in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the class type for the linear layer based on bit precision.\n",
    "    linear_class = bnb.nn.Linear4bit if bit_precision == 4 else bnb.nn.Linear8bitLt if bit_precision == 8 else torch.nn.Linear\n",
    "\n",
    "    linear_layer_names = set()  # Set to store unique names of linear layers.\n",
    "\n",
    "    # Iterate through all modules in the model to find instances of the specified linear layer class.\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, linear_class):\n",
    "            # Extract the base or last segment of the module name and add it to the set.\n",
    "            module_name_segment = name.split('.')[0 if name.count('.') == 0 else -1]\n",
    "            linear_layer_names.add(module_name_segment)\n",
    "\n",
    "    # Remove 'lm_head' from the set if present, typically applicable for 16-bit precision models.\n",
    "    linear_layer_names.discard('lm_head')\n",
    "\n",
    "    return list(linear_layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29616afa-b38e-4a8c-958f-9ead1c349fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k_proj', 'v_proj', 'o_proj', 'gate_proj', 'q_proj', 'up_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = get_linear_layer_names(base_model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b6115-677d-4404-8450-7a2f41e7150f",
   "metadata": {},
   "source": [
    "## 14: Configuring and Applying LORA to the Base Model\n",
    "This step involves configuring and applying Low-Rank Adaptation (LORA) to the base model using the `peft` package. A `LoraConfig` object is created, specifying various parameters such as the rank of LORA layers (`r`), a scaling factor (`lora_alpha`), target modules for adaptation, dropout rate for regularization (`lora_dropout`), bias settings, and the task type (Causal Language Modeling). Then, using the `get_peft_model` function, these LORA adaptations are applied to the specified target modules in the base model, enhancing it for Parameter-Efficient Fine-Tuning (PEFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9e8ce3-d94d-4387-b634-6c1920cbfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model  # Importing LoraConfig and get_peft_model from the 'peft' package.\n",
    "\n",
    "# Create a configuration object for LORA (Low-Rank Adaptation) layers.\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 'r' is the rank of the LORA layers, affecting the amount of parameters added.\n",
    "    lora_alpha=32,  # 'lora_alpha' is a scaling factor for LORA's low-rank matrices.\n",
    "    target_modules=modules,  # 'target_modules' are the modules in the model to be adapted by LORA.\n",
    "    lora_dropout=0.05,  # 'lora_dropout' specifies the dropout rate in LORA layers for regularization.\n",
    "    bias=\"none\",  # 'bias' determines the usage of bias in LORA layers, here set to 'none'.\n",
    "    task_type=\"CAUSAL_LM\"  # 'task_type' specifies the type of task, here set to causal language modeling.\n",
    ")\n",
    "\n",
    "# Enhance the base model with PEFT (Parameter-Efficient Fine-Tuning) using the specified LORA configuration.\n",
    "# This process involves applying the LORA adaptations to the specified target modules in the model.\n",
    "base_model = get_peft_model(base_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "864e49ed-875f-4b25-a6d6-4480c4f40b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of trainable and total parameters in the base model.\n",
    "# trainable, total = base_model.get_nb_trainable_parameters()\n",
    "\n",
    "# Print out the number of trainable parameters, total parameters, \n",
    "# and the percentage of parameters that are trainable.\n",
    "# This provides insight into the proportion of the model that can be updated during training.\n",
    "# print(f\"Trainable: {trainable} | Total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31f444-83ff-41e0-8b07-124b4d532268",
   "metadata": {},
   "source": [
    "## 15: Initializing the Supervised Fine-Tuning Trainer\n",
    "This step involves initializing the Supervised Fine-Tuning (SFT) Trainer for a transformer model. The process includes configuring the tokenizer, clearing the GPU cache for efficient memory usage, and setting up the trainer with specific parameters for model training and evaluation, PEFT configuration using LORA, training arguments like batch size, learning rate, and logging settings. The setup is tailored for language model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a1ea29-e992-4173-80ee-f0b8f83a6e10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3528a16a352484e8b5bff2b535031ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b25a17bb06945b4a2567fcdf4df7591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27565 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import transformers  # Importing the transformers library, which provides tools for working with transformer models.\n",
    "\n",
    "from trl import SFTTrainer  # Importing SFTTrainer from the trl (transformer reinforcement learning) package.\n",
    "# from transformers import TrainerArgument\n",
    "# from transformers import AutoModelWithLMHead, AutoTokenizer, top_k_top_p_filtering\n",
    "\n",
    "# Setting the padding token of the tokenizer to be the same as its end-of-sentence token.\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# Clearing the GPU cache to free up memory and avoid potential out-of-memory issues.\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Initializing the Supervised Fine-Tuning (SFT) Trainer.\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,  # The model to be fine-tuned.\n",
    "    train_dataset=train_data,  # The dataset for training.\n",
    "    eval_dataset=test_data,  # The dataset for evaluation.\n",
    "    dataset_text_field=\"prompt\",  # The field in the dataset that contains the text to be processed.\n",
    "    peft_config=lora_config,  # The PEFT (Parameter-Efficient Fine-Tuning) configuration, here using LORA.\n",
    "    args=transformers.TrainingArguments(  # Configuration for the training process.\n",
    "        per_device_train_batch_size=10,  # Batch size per device.\n",
    "        gradient_accumulation_steps=16,  # Number of steps to accumulate gradients before updating model weights.\n",
    "        warmup_steps=50,  # Absolute number of warmup steps for the learning rate scheduler.\n",
    "        max_steps=100,  # Maximum number of training steps, -1 means unlimited.\n",
    "        learning_rate=1e-5,  # The learning rate for optimization.\n",
    "        logging_dir=\"./logs\",  # Directory where training logs will be stored.\n",
    "        logging_first_step=True,  # Log the first training step, useful for debugging.\n",
    "        logging_steps=20,  # Frequency of logging training information.\n",
    "        evaluation_strategy=\"steps\",  # Strategy to perform model evaluation.\n",
    "        optim=\"adamw_torch\",  # The optimizer to be used.\n",
    "        eval_steps=50,  # Number of steps before performing evaluation.\n",
    "        output_dir=\"/opt/app-root/src/data/v8-finance-3/outputs\",  # Directory to store output files.\n",
    "        load_best_model_at_end=True,  # Whether to load the best model at the end of training.\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(base_tokenizer, mlm=False),  # Data collator for language modeling.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc80bdb-f837-4283-ad93-b28346a47c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 7:49:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.160300</td>\n",
       "      <td>1.994920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.812900</td>\n",
       "      <td>1.801070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.0092516231536863, metrics={'train_runtime': 28331.2399, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.004, 'total_flos': 3.472788689264886e+17, 'train_loss': 2.0092516231536863, 'epoch': 2.32})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable caching in the model's configuration. \n",
    "# This is typically done during training to save memory, as caching activations is not necessary.\n",
    "# However, for inference, caching should be re-enabled for improved performance.\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# Start the training process using the SFTTrainer instance.\n",
    "# The function argument specifies the path to resume training from a specific checkpoint.\n",
    "# trainer.train(\"/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000\")\n",
    "# Initial Training without checkpoint\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721b55f-fcd2-4997-a7f8-a224d6158fc4",
   "metadata": {},
   "source": [
    "## 16: Publishing Model and Tokenizer to Hugging Face Model Hub\n",
    "This step involves uploading the fine-tuned model and its associated tokenizer to the Hugging Face Model Hub. The model and tokenizer are pushed to the hub under the repository name `\"Llama-2-7b-hf_finetuned_finance_jupyter\"`, making them accessible online for public use and download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01f6772e-ade0-472e-b76e-744f43d0787f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bc2bcbb13f40e89ca60ef55cd12cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/80.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter/commit/930a84b13abc3df1cd7abd430a6376cda99c0e73', commit_message='Upload tokenizer', commit_description='', oid='930a84b13abc3df1cd7abd430a6376cda99c0e73', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the fine-tuned model to the Hugging Face Model Hub.\n",
    "# This makes the model available online for others to use and download.\n",
    "# \"Llama-2-7b-hf_finetuned_finance_jupyter_v7\" is the repository name on the Model Hub.\n",
    "base_model.push_to_hub(\"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\")\n",
    "\n",
    "# Similarly, push the tokenizer associated with the fine-tuned model to the Hugging Face Model Hub.\n",
    "# This ensures that users who download the model also have access to the correct tokenizer.\n",
    "# The repository name is kept the same for consistency and easy association with the model.\n",
    "base_tokenizer.push_to_hub(\"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74925f9-3f7e-4e79-a232-e5eb1b02fd0f",
   "metadata": {},
   "source": [
    "## 17: Loading and Utilizing a PEFT Enhanced Language Model\n",
    "The code involves loading a fine-tuned language model enhanced with Parameter-Efficient Fine-Tuning (PEFT) and its tokenizer from the Hugging Face Model Hub, using a specified identifier. The PEFT-enhanced model is loaded with settings for efficiency, including 4-bit loading and automatic device placement. After loading, the model and tokenizer are used to generate a response to a sample query, showcasing the practical application of the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ef37c-1b3e-4f7d-b3f3-734816a3a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea204d50708a43e888166f026643f4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cda7ab089140d995b3539a971b53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac8b78b81234d29945421dad5d9f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/80.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig  # Import PEFT (Parameter-Efficient Fine-Tuning) related classes.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Importing necessary classes from transformers.\n",
    "\n",
    "# Identifier for the fine-tuned PEFT model on the Hugging Face Model Hub.\n",
    "peft_model_id = \"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\"\n",
    "\n",
    "# Load the PEFT configuration from the Hugging Face Model Hub using the model identifier.\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base causal language model specified in the PEFT config, enabling 4-bit loading for efficiency.\n",
    "# 'device_map=\"auto\"' automatically places the model on the most appropriate device (CPU/GPU).\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map='auto')\n",
    "\n",
    "# Load the tokenizer corresponding to the base model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the PEFT model from the pretrained model and config, enabling the use of PEFT enhancements.\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ad9d2-6313-4ee1-87c4-7d5e73aa0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
      "\n",
      "    ### Instruction:\n",
      "    Will capital gains affect my tax bracket?\n",
      "\n",
      "    ### Completion:\n",
      "    \n",
      "    ### Explanation:\n",
      "\n",
      "    ### Hint:\n",
      "    Capital gains are taxed differently from ordinary income. Capital gains are taxed at lower rates than ordinary income.\n",
      "\n",
      "    ### Source:\n",
      "    https://www.irs.gov/businesses/small-businesses-self-employed/capital-gains-and-losses\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "How much tax do I owe?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n",
      "\n",
      "### Completion:\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "### Hint:\n",
      "\n",
      "### Source:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "What is the difference between a 1099 and a W2?\n"
     ]
    }
   ],
   "source": [
    "result = create_model_response(task_query=\"Will capital gains affect my tax bracket?\", inference_model=model, sequence_tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013553a-b5fe-4f48-831d-1de0afd26079",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the trainer state data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     10\u001b[0m     trainer_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the metrics, ensuring we only take steps that have both a loss and learning rate\u001b[39;00m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's load the JSON data from the file\n",
    "file_path =  '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'\n",
    "\n",
    "\n",
    "# Load the trainer state data\n",
    "with open(file_path, 'r') as file:\n",
    "    trainer_state = json.load(file)\n",
    "\n",
    "# Extract the metrics, ensuring we only take steps that have both a loss and learning rate\n",
    "steps_with_loss = [log_entry['step'] for log_entry in trainer_state['log_history'] if 'loss' in log_entry]\n",
    "losses = [log_entry['loss'] for log_entry in trainer_state['log_history'] if 'loss' in log_entry]\n",
    "\n",
    "steps_with_lr = [log_entry['step'] for log_entry in trainer_state['log_history'] if 'learning_rate' in log_entry]\n",
    "learning_rates = [log_entry['learning_rate'] for log_entry in trainer_state['log_history'] if 'learning_rate' in log_entry]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for Loss vs. Steps with loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps_with_loss, losses, label='Loss', color='red')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Learning Rate vs. Steps with learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps_with_lr, learning_rates, label='Learning Rate', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7d3bb-80a7-4d48-b329-06af994f887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "file_path = '/opt/app-root/src/data/v8-finance-3/outputs/checkpoint-9000/trainer_state.json'\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    trainer_state_data = json.load(file)\n",
    "\n",
    "# Normalize the nested 'log_history' data into a flat table\n",
    "df = pd.json_normalize(trainer_state_data, record_path=['log_history'])\n",
    "\n",
    "# Filter out the required columns\n",
    "filtered_df = df[['epoch', 'learning_rate', 'loss', 'step']].dropna()\n",
    "\n",
    "# Save the filtered data as a CSV file\n",
    "csv_file_path = '/opt/app-root/src/data/training_data.csv'\n",
    "filtered_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ea303-37c7-409f-9361-f73a29713329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
