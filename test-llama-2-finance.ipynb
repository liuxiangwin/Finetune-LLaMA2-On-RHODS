{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf140b5-ba39-4dca-b317-213161827eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet --quiet\n",
    "!pip install -U bitsandbytes --quiet\n",
    "!pip install  -U git+https://github.com/huggingface/peft.git --quiet\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install scipy --quiet\n",
    "!pip install ipywidgets --quiet\n",
    "!pip install transformers==4.36.0 --quiet\n",
    "!pip install trl==0.7.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db001838-be37-4f58-805f-1b2357d9ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cca5e1d24534cd9a1eba8c3fb3eae5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig  # Import PEFT (Parameter-Efficient Fine-Tuning) related classes.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Importing necessary classes from transformers.\n",
    "\n",
    "# Identifier for the fine-tuned PEFT model on the Hugging Face Model Hub.\n",
    "peft_model_id = \"redhat-model-finetuing/Llama-2-7b-hf_finetuned_finance_jupyter\"\n",
    "\n",
    "# Load the PEFT configuration from the Hugging Face Model Hub using the model identifier.\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base causal language model specified in the PEFT config, enabling 4-bit loading for efficiency.\n",
    "# 'device_map=\"auto\"' automatically places the model on the most appropriate device (CPU/GPU).\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, \n",
    "                                             return_dict=True, \n",
    "                                             load_in_4bit=True, \n",
    "                                             device_map='auto')\n",
    "\n",
    "# Load the tokenizer corresponding to the base model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the PEFT model from the pretrained model and config, enabling the use of PEFT enhancements.\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5af9e3-13fd-44a0-b9fe-e39dd0842a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_response(task_query: str, inference_model, sequence_tokenizer) -> str:\n",
    "    processing_device = \"cuda:0\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
    "\n",
    "    ### Instruction:\n",
    "    {user_task_query}\n",
    "\n",
    "    ### Completion:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the template with the user's query\n",
    "    task_prompt = prompt_template.format(user_task_query=task_query)\n",
    "\n",
    "    # Encoding the prompt for the model\n",
    "    encoded_input = sequence_tokenizer(task_prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Sending the encoded input to the designated processing device\n",
    "    model_input_tensor = encoded_input.to(processing_device)\n",
    "\n",
    "    # Generating tokens from the model based on the input\n",
    "    generated_token_ids = inference_model.generate(\n",
    "        **model_input_tensor, \n",
    "        max_new_tokens=1000, \n",
    "        do_sample=True, \n",
    "        pad_token_id=sequence_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decoding the generated tokens to form the response\n",
    "    generated_response = sequence_tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92fb8b5-1fa3-4098-8039-6ee46275a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What is the bank's futures rate ?\"\n",
    "\n",
    "\n",
    "# prompt=\"How do capital gains influence my income tax rate ?\"\n",
    "\n",
    "# prompt=\"Will capital gains affect my tax bracket?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78aa82c7-ca0b-489b-9035-03aa266b1277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Here is a task that requires an informative response. Please complete the task based on the provided instruction.\n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n",
      "    ### Completion:\n",
      "    5.42\n",
      "\n",
      "    ### Explanation:\n",
      "    \n",
      "    The futures rate is the interest rate that is expected to be paid in the future. \n",
      "\n",
      "    ### Instruction:\n",
      "    What is the bank's futures rate ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = create_model_response(task_query=prompt, \n",
    "                               inference_model=model, \n",
    "                               sequence_tokenizer=tokenizer)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
